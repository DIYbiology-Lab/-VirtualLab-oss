version: "3.9"

x-common-env: &common_env
  TZ: Asia/Tokyo
  VLLM_API_KEY: local-oss
  MODEL_ID: ${MODEL_ID-openai/gpt-oss-20b}   # 20b/120b を対話で上書き
  SERVED_MODEL_NAME: ${MODEL_ID-openai/gpt-oss-20b}

services:
  # A) 初回DL（軽量）
  llm-runtime:
    build:
      context: ./llm
      dockerfile: Dockerfile.runtime
      args:
        MODEL_ID: ${MODEL_ID-openai/gpt-oss-20b}
    environment: *common_env
    ports: ["8000:8000"]
    volumes:
      - ./hf-cache:/root/.cache/huggingface
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/v1/models -H 'Authorization: Bearer ${VLLM_API_KEY}' >/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 30
    deploy:
      resources:
        reservations:
          devices: [{ driver: nvidia, count: 1, capabilities: [gpu] }]
    profiles: ["runtime"]

  vlab-runtime:
    build:
      context: ./vlab
      dockerfile: Dockerfile.vlab
    environment:
      <<: *common_env
      OPENAI_BASE_URL: http://llm-runtime:8000/v1
      OPENAI_API_KEY: ${VLLM_API_KEY-local-oss}
    depends_on: [llm-runtime]
    volumes:
      - ./work:/workspace
    ports: ["8888:8888"]
    profiles: ["runtime"]

  # B) 重み同梱（完全オフライン）
  llm-bundled:
    build:
      context: ./llm
      dockerfile: Dockerfile.weights
      args:
        MODEL_ID: ${MODEL_ID-openai/gpt-oss-20b}
    environment: *common_env
    ports: ["8000:8000"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/v1/models -H 'Authorization: Bearer ${VLLM_API_KEY}' >/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 30
    deploy:
      resources:
        reservations:
          devices: [{ driver: nvidia, count: 1, capabilities: [gpu] }]
    profiles: ["bundled"]

  vlab-bundled:
    build:
      context: ./vlab
      dockerfile: Dockerfile.vlab
    environment:
      <<: *common_env
      OPENAI_BASE_URL: http://llm-bundled:8000/v1
      OPENAI_API_KEY: ${VLLM_API_KEY-local-oss}
    depends_on: [llm-bundled]
    volumes:
      - ./work:/workspace
    ports: ["8888:8888"]
    profiles: ["bundled"]